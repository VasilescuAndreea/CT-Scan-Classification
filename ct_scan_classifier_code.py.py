# -*- coding: utf-8 -*-
"""CT SCAN CLASSIFIRE- FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkPpmrKUKcAhExUSxWjIdi6xw1J_BGTs
"""

from google.colab import drive
# drive.mount('/content/gdrive', force_remount=True)
drive.mount('/content/gdrive')

!unzip "/content/gdrive/MyDrive/arhiva.zip" -d docs/

# Commented out IPython magic to ensure Python compatibility.
# % cd docs

! ls



!ln -s //content/data/data/ai-unibuc-23-31-2021
! ls /content

!ls /content/docs/ai-unibuc-23-31-2021

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/docs/ai-unibuc-23-31-2021"

import numpy as np
import pandas as pd
import os
import torch

df = pd.read_csv("/content/docs/ai-unibuc-23-31-2021/train.txt", header = None, names = ["path", "label"])
df.head()

#datagen = keras.preprocessing.image.ImageDataGenerator()
#train_it = datagen.flow_from_directory('/content/ai-unibuc-23-31-2021/train', class_mode=None)

#import tensorflow as tf 
#import tensorflow.keras as keras 
#images = keras.preprocessing.image_dataset_from_directory('/content/ai-unibuc-23-31-2021/train')

images_paths = df.loc[0:15000, "path"].tolist()
images_labels = df.loc[0:15000, "label"].tolist()
images_paths = [os.path.join('train',path) for path in images_paths]

len(images_paths)

images_paths

from PIL import Image
import matplotlib.pyplot as plt

# -1 ca sa vad daca am toate pozele, o verific pe ultima
image = Image.open(images_paths[-1])
plt.imshow(image, cmap='gray')

from torch.utils.data import Dataset, DataLoader

!pip install pycuda

import pdb
class TMDataset(Dataset):
  def __init__(self, images_paths, images_labels):
    self.images_paths = images_paths
    self.images_labels = images_labels
  
  def __len__(self):
    return len(self.images_paths)

  def __getitem__(self, index):
    image_path = self.images_paths[index]
    image = Image.open(image_path)
    image = np.array(image) / 255
    torch_image = torch.tensor(image).unsqueeze(0).float()
    label = torch.tensor(self.images_labels[index])

    return torch_image, label

datapoints = []
for i in range(len(images_labels)):
  if(images_labels[i] == 2):
    datapoints.append(i)
imagepaths2 = []
imagelabels2 = []
for i in datapoints:
  imagepaths2.append(images_paths[i])
  imagelabels2.append(images_labels[i])

imagepaths2 = images_paths
imagelables2 = images_labels

tmdataset = TMDataset(images_paths, images_labels)
tmdataset[-1]

# my_dataloader = DataLoader(tmdataset, batch_size = 128 , shuffle = False, prefetch_factor=4, num_workers=4)
my_dataloader = DataLoader(tmdataset, batch_size = 40 , shuffle = False, prefetch_factor=4, num_workers=2)

for batch in my_dataloader:
  tensim = batch[0][0][0]
  plt.imshow(tensim)
  break

import torch.nn as nn
import torch.nn.functional as F
if torch.cuda.is_available():
  dev = "cuda:0"
else:
  dev = "cpu"
device = torch.device(dev)
print(dev)

class MyConvolutionNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.first_convolution = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)
        self.second_convolution = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2)
        
        self.pool = nn.MaxPool2d(2, 2)
        # transform dintr o imagine 3D intru un vector de rezultate
        self.flatten = nn.Flatten()
        
        # fc -> layere neuronale normale
        self.fully_connected1 = nn.Linear(1600, 50)
        self.fully_connected2 = nn.Linear(50, 3)
        # self.fully_connected1 = nn.Linear(1600, 1200)
        # self.fully_connected2 = nn.Linear(1200, 800)
        # self.fully_connected3 = nn.Linear(800, 3)
        
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # to use BatchNorm2D if needed
        # to use dropout on fc
        x = self.first_convolution(x)
        x = self.relu(x)
        x = self.pool(x)
        
        x = self.second_convolution(x)
        x = self.relu(x)
        x = self.pool(x)
        
        x = self.flatten(x)
        x = self.fully_connected1(x)
        x = self.relu(x)
        
        x = self.fully_connected2(x)
        x = self.relu(x)
        x = self.softmax(x)

        #x = self.fully_connected3(x)
        #x = self.relu(x)
        #x = self.softmax(x)
        return x

import torch.optim as optim

net = MyConvolutionNetwork()
net.to(device)
optimizer = optim.Adam(net.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
# criterion.to(device)

# this is one epoch, it should be done for more epochs

net.train() # if training mode
# net.eval() # if validation mode
# se antreneaza de 1000 de ori pe setul de date
for i in range(0, 1):
  print('epoca' + str(i))
  j = 0
  for batch in my_dataloader:
    # print(batch)
    j += 1
    # print('batch',j)
    torch_image, label = batch
    # torch_image.to(device)
    torch_image = torch_image.cuda()
    # label.to(device)
    label = label.cuda()
    # print('uploaded to gpu')
    
    optimizer.zero_grad()
    # output = net(torch_image.cuda())
    output = net(torch_image)
    # print('ran net')
    
    predicted_label = torch.argmax(output, axis=1)
    print("Label: ", label)
    print("Prediction: ", predicted_label)
    # predicted_label.to(device)
    # output este o lista de lista

    # output.to(device)
    # label.to(device)
    # label = label.cuda()
    loss = criterion(output, label)
    # print('calculated loss')
    
    # just on training -- ac
    loss.backward()
    optimizer.step()
    # print('got gradient')
    
    #print(loss)
print("Done")

net = MyConvolutionNetwork()
net.to(device)
optimizer = optim.Adam(net.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
# criterion.to(device)
for layer in net.children():
  if hasattr(layer, 'reset_parameters'):
    layer.reset_parameters()

# this is one epoch, it should be done for more epochs

net.train() # if training mode
# net.eval() # if validation mode
# se antreneaza de 1000 de ori pe setul de date
for i in range(0, 1):
  print('epoca' + str(i))
  j = 0
  for batch in my_dataloader:
    # print(batch)
    j += 1
    # print('batch',j)
    torch_image, label = batch
    # torch_image.to(device)
    #torch_image = torch_image.cuda()
    # label.to(device)
    label = label.cuda()
    # print('uploaded to gpu')
    
    optimizer.zero_grad()
    output = net(torch_image.cuda())
    #output = net(torch_image)
    loss = criterion(output, label)
    # print('ran net')
    predicted_label = torch.argmax(output, axis=1)
    if i == 0:
      print("Label: ", label)
      print("Prediction: ", predicted_label)
    #predicted_label.to(device)
    # output este o lista de lista

    #output.to(device)
    #label.to(device)
    # label = label.cuda()
    
    # print('calculated loss')
    
    # just on training -- ac
    loss.backward()
    optimizer.step()
    # print('got gradient')
    
    #print(loss)
print("Done")

# import torch
# torch.cuda.is_available

###### TEST

df = pd.read_csv("/content/docs/ai-unibuc-23-31-2021/validation.txt", header = None, names = ["path", "label"])
df.head()

validation_images_paths = df.loc[0:4500, "path"].tolist()
validation_images_labels = df.loc[0:4500, "label"].tolist()
validation_images_paths = [os.path.join('/content/docs/ai-unibuc-23-31-2021/validation',path) for path in validation_images_paths]

len(validation_images_paths)

validation_tmdataset = TMDataset(validation_images_paths, validation_images_labels)
validation_tmdataset[-1]

validation_dataloader = DataLoader(validation_tmdataset, batch_size = 128 , shuffle = False, prefetch_factor=4, num_workers=4)

net

for i in range(1, 300):
  print('epoca' + str(i))
  j = 0
  for batch in my_dataloader:
    # print(batch)
    j += 1
    # print('batch',j)
    torch_image, label = batch
    # torch_image.to(device)
    torch_image = torch_image.cuda()
    # label.to(device)
    label = label.cuda()
    # print('uploaded to gpu')
    
    optimizer.zero_grad()
    # output = net(torch_image.cuda())
    output = net(torch_image)
    # print('ran net')
    
    predicted_label = torch.argmax(output, axis=1)
    #print("Label: ", label)
    #print("Prediction: ", predicted_label)
    # predicted_label.to(device)
    # output este o lista de lista

    # output.to(device)
    # label.to(device)
    # label = label.cuda()
    loss = criterion(output, label)
    # print('calculated loss')
    
    # just on training -- ac
    loss.backward()
    optimizer.step()
    # print('got gradient')
    
    print(loss)
print("Done")

#net = MyConvolutionNetwork()
#net.to(device)
#optimizer = optim.Adam(net.parameters(), lr=0.001)
#criterion = nn.CrossEntropyLoss()
# criterion.to(device)

# this is one epoch, it should be done for more epochs

# net.train() # if training mode
net.eval() # if validation mode
# se antreneaza de 1000 de ori pe setul de date

j = 0
validation_predicted_label = []
for batch in validation_dataloader:
  # print(batch)
  #j += 1
  # print('batch',j)
  torch_image, validation_label = batch
  torch_image = torch_image.to(device)
  # validation_label.to(device)
  # print('uploaded to gpu')
  # validation_label = validation_label.to(device)
    
  #optimizer.zero_grad()
  # output = net(torch_image.cuda())
  output = net(torch_image)
  # print('ran net')
  #print(torch.argmax(output, axis=1).cpu().numpy())
  validation_predicted_label = np.concatenate((validation_predicted_label, torch.argmax(output, axis=1).cpu().numpy()))
  # validation_predicted_label.to(device)
  # output.to(device)
  # validation_label.to(device)
  # validation_label = validation_label.cuda()
  # print('calculated loss', loss)
    
  # just on training -- ac
  # loss.backward()
  # optimizer.step()
  # print('got gradient')
    
  # print(loss)
print("end")



# nb_classes = 9

# confusion_matrix = torch.zeros(nb_classes, nb_classes)
# with torch.no_grad():
#     for i, (inputs, classes) in enumerate(dataloaders['val']):
#         inputs = inputs.to(device)
#         classes = classes.to(device)
#         outputs = model_ft(inputs)
#         _, preds = torch.max(outputs, 1)
#         for t, p in zip(validation_label.view(-1), validation_predicted_label.view(-1)):
#                 confusion_matrix[t.long(), p.long()] += 1

# print(confusion_matrix)

# import sklearn.metrics.accuracy_score
from sklearn.metrics import accuracy_score
validation_label = validation_predicted_label
print(validation_images_labels)
print(validation_label.shape)
accuracy = accuracy_score(validation_images_labels, validation_label)
print(accuracy)

import copy
zerocincidoi = copy.deepcopy(validation_label)
zerocincidoi2 = copy.deepcopy(validation_images_labels)

validation_label[0] = 2
print(validation_label[:10])
print(zerocincidoi[:10])

print(validation_label)
print(validation_images_labels)

import copy
testfile = pd.read_csv("/content/docs/ai-unibuc-23-31-2021/test.txt", header = None, names = ["path"])
testfile.head()

#print(testfile.size)
test_images_paths = testfile.loc[0:3900, "path"].tolist()
test_images_paths_relative = copy.deepcopy(test_images_paths)
#print(test_images_paths[:10])

test_images_paths = [os.path.join('/content/docs/ai-unibuc-23-31-2021/test',path) for path in test_images_paths]
#print(test_images_paths[:10])
#img = Image.open(test_images_paths[0])
#plt.imshow(img)
test_labels = [0] * 3900
testdataset = TMDataset(test_images_paths, test_labels)
testdataset[-1]
testdataloader = DataLoader(testdataset, batch_size = 128 , shuffle = False, prefetch_factor=4, num_workers=4)
test_predicted_label = []
for batch in testdataloader:
  torch_image, validation_label = batch
  torch_image = torch_image.to(device)
  # validation_label.to(device)
  # print('uploaded to gpu')
  # validation_label = validation_label.to(device)
    
  #optimizer.zero_grad()
  # output = net(torch_image.cuda())
  output = net(torch_image)
  # print('ran net')
  #print(torch.argmax(output, axis=1).cpu().numpy())
  test_predicted_label = np.concatenate((test_predicted_label, torch.argmax(output, axis=1).cpu().numpy()))



for i in range(len(test_predicted_label)):
  print(test_predicted_label[i])